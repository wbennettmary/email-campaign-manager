#!/usr/bin/env python3
"""
Aggressive Performance Email Campaign Manager
Forces maximum resource utilization and handles 100+ concurrent campaigns
"""

import os
import json
import time
import threading
import multiprocessing
import psutil
from datetime import datetime, timedelta
from functools import wraps
import logging
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import queue
import gc
import signal
import sys
import mmap
import tempfile
import shutil
from collections import defaultdict, deque
import weakref

from flask import Flask, render_template, request, jsonify, redirect, url_for, flash, session, send_file
from flask_socketio import SocketIO, emit
from flask_login import LoginManager, login_user, logout_user, login_required, current_user, UserMixin
from werkzeug.security import generate_password_hash, check_password_hash
from werkzeug.utils import secure_filename

# Disable all logging for maximum performance
logging.getLogger().setLevel(logging.ERROR)
logging.getLogger('werkzeug').setLevel(logging.ERROR)
logging.getLogger('socketio').setLevel(logging.ERROR)
logging.getLogger('engineio').setLevel(logging.ERROR)

# Flask app configuration
app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
app.config['SOCKETIO_ASYNC_MODE'] = 'threading'
app.config['MAX_CONTENT_LENGTH'] = 2000 * 1024 * 1024  # 2GB max file size

# SocketIO with maximum performance
socketio = SocketIO(
    app, 
    cors_allowed_origins="*", 
    async_mode='threading', 
    logger=False,
    engineio_logger=False,
    ping_timeout=300,
    ping_interval=60,
    max_http_buffer_size=2e9
)

# Login manager
login_manager = LoginManager()
login_manager.init_app(app)
login_manager.login_view = 'login'

# Global configuration
ACCOUNTS_FILE = 'accounts.json'
CAMPAIGNS_FILE = 'campaigns.json'
USERS_FILE = 'users.json'
DATA_LISTS_FILE = 'data_lists.json'
CAMPAIGN_LOGS_FILE = 'campaign_logs.json'
NOTIFICATIONS_FILE = 'notifications.json'

# Get system resources for aggressive utilization
CPU_COUNT = multiprocessing.cpu_count()
TOTAL_RAM = psutil.virtual_memory().total
AVAILABLE_RAM = psutil.virtual_memory().available

print(f"ðŸš€ System Resources: {CPU_COUNT} CPU cores, {TOTAL_RAM // (1024**3)}GB RAM")
print(f"ðŸŽ¯ Target: Use {CPU_COUNT * 8} workers, {TOTAL_RAM * 0.8 // (1024**3)}GB RAM")

# Aggressive thread pools - Use ALL available resources
CAMPAIGN_WORKERS = CPU_COUNT * 10  # 10x CPU cores
EMAIL_WORKERS = CPU_COUNT * 20     # 20x CPU cores  
DATA_WORKERS = CPU_COUNT * 5       # 5x CPU cores
STATS_WORKERS = CPU_COUNT * 2      # 2x CPU cores
FILE_WORKERS = CPU_COUNT * 3       # 3x CPU cores

print(f"ðŸ”¥ Creating {CAMPAIGN_WORKERS + EMAIL_WORKERS + DATA_WORKERS + STATS_WORKERS + FILE_WORKERS} total workers")

campaign_executor = ThreadPoolExecutor(max_workers=CAMPAIGN_WORKERS)
email_executor = ThreadPoolExecutor(max_workers=EMAIL_WORKERS)
data_executor = ThreadPoolExecutor(max_workers=DATA_WORKERS)
stats_executor = ThreadPoolExecutor(max_workers=STATS_WORKERS)
file_executor = ThreadPoolExecutor(max_workers=FILE_WORKERS)

# Aggressive cache with maximum size
class AggressiveCache:
    def __init__(self):
        self._cache = {}
        self._timestamps = {}
        self._lock = threading.RLock()
        self._max_size = 100000  # Massive cache
        self._cleanup_thread = threading.Thread(target=self._cleanup_loop, daemon=True)
        self._cleanup_thread.start()
        
        # Preload all data aggressively
        self._preload_thread = threading.Thread(target=self._preload_all_data, daemon=True)
        self._preload_thread.start()
    
    def get(self, key, default=None):
        with self._lock:
            if key in self._cache:
                if time.time() - self._timestamps[key] < 7200:  # 2 hour TTL
                    return self._cache[key]
                else:
                    del self._cache[key]
                    del self._timestamps[key]
            return default
    
    def set(self, key, value, ttl=7200):
        with self._lock:
            if len(self._cache) >= self._max_size:
                oldest_key = min(self._timestamps.keys(), key=lambda k: self._timestamps[k])
                del self._cache[oldest_key]
                del self._timestamps[oldest_key]
            
            self._cache[key] = value
            self._timestamps[key] = time.time()
    
    def invalidate(self, key):
        with self._lock:
            if key in self._cache:
                del self._cache[key]
                del self._timestamps[key]
    
    def _cleanup_loop(self):
        while True:
            time.sleep(600)  # Cleanup every 10 minutes
            current_time = time.time()
            with self._lock:
                expired_keys = [k for k, v in self._timestamps.items() if current_time - v > 7200]
                for key in expired_keys:
                    del self._cache[key]
                    del self._timestamps[key]
    
    def _preload_all_data(self):
        """Aggressively preload all data"""
        try:
            files = [ACCOUNTS_FILE, CAMPAIGNS_FILE, USERS_FILE, DATA_LISTS_FILE]
            for file_path in files:
                if os.path.exists(file_path):
                    cache_key = f"file_{file_path}"
                    data = self._read_file_sync(file_path)
                    self.set(cache_key, data, ttl=7200)
                    print(f"ðŸ“¦ Preloaded {file_path}: {len(data)} items")
        except Exception as e:
            print(f"Error in data preload: {e}")
    
    def _read_file_sync(self, file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return {}

# Global cache
cache = AggressiveCache()

# Campaign execution tracking
campaign_execution_tracking = {}
execution_lock = threading.RLock()

# Rate limiting with aggressive defaults
rate_limit_data = defaultdict(lambda: {
    'last_send_time': {},
    'daily_count': defaultdict(int),
    'hourly_count': defaultdict(int),
    'minute_count': defaultdict(int),
    'second_count': defaultdict(int)
})

# Aggressive file operations
def read_json_file_aggressive(filename, default=None):
    """Aggressive JSON file reading with maximum caching"""
    cache_key = f"file_{filename}"
    data = cache.get(cache_key)
    if data is not None:
        return data
    
    try:
        # Use memory mapping for all files
        if os.path.exists(filename):
            with open(filename, 'r+b') as f:
                mm = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)
                data = json.loads(mm.read().decode('utf-8'))
                mm.close()
        else:
            data = {}
        
        cache.set(cache_key, data, ttl=7200)  # 2 hour cache
        return data
    except Exception as e:
        print(f"Error reading {filename}: {e}")
        return default or {}

def write_json_file_aggressive(filename, data):
    """Aggressive JSON file writing"""
    try:
        temp_filename = filename + '.tmp'
        with open(temp_filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        os.replace(temp_filename, filename)
        cache.invalidate(f"file_{filename}")
        return True
    except Exception as e:
        print(f"Error writing {filename}: {e}")
        return False

# Aggressive data manager
class AggressiveDataManager:
    def __init__(self):
        self._data = {}
        self._last_refresh = {}
        self._lock = threading.RLock()
        self._refresh_interval = 30  # 30 seconds
        self._background_thread = threading.Thread(target=self._background_refresh, daemon=True)
        self._background_thread.start()
    
    def _should_refresh(self, data_type):
        return time.time() - self._last_refresh.get(data_type, 0) > self._refresh_interval
    
    def get_accounts(self):
        with self._lock:
            if self._should_refresh('accounts'):
                self._data['accounts'] = read_json_file_aggressive(ACCOUNTS_FILE, {})
                self._last_refresh['accounts'] = time.time()
            return self._data.get('accounts', {})
    
    def get_campaigns(self):
        with self._lock:
            if self._should_refresh('campaigns'):
                self._data['campaigns'] = read_json_file_aggressive(CAMPAIGNS_FILE, {})
                self._last_refresh['campaigns'] = time.time()
            return self._data.get('campaigns', {})
    
    def get_users(self):
        with self._lock:
            if self._should_refresh('users'):
                self._data['users'] = read_json_file_aggressive(USERS_FILE, {})
                self._last_refresh['users'] = time.time()
            return self._data.get('users', {})
    
    def get_data_lists(self):
        with self._lock:
            if self._should_refresh('data_lists'):
                self._data['data_lists'] = read_json_file_aggressive(DATA_LISTS_FILE, {})
                self._last_refresh['data_lists'] = time.time()
            return self._data.get('data_lists', {})
    
    def save_accounts(self, accounts):
        with self._lock:
            self._data['accounts'] = accounts
            self._last_refresh['accounts'] = time.time()
        return write_json_file_aggressive(ACCOUNTS_FILE, accounts)
    
    def save_campaigns(self, campaigns):
        with self._lock:
            self._data['campaigns'] = campaigns
            self._last_refresh['campaigns'] = time.time()
        return write_json_file_aggressive(CAMPAIGNS_FILE, campaigns)
    
    def save_users(self, users):
        with self._lock:
            self._data['users'] = users
            self._last_refresh['users'] = time.time()
        return write_json_file_aggressive(USERS_FILE, users)
    
    def _background_refresh(self):
        """Aggressive background refresh"""
        while True:
            try:
                self.get_accounts()
                self.get_campaigns()
                self.get_users()
                self.get_data_lists()
                time.sleep(15)  # Refresh every 15 seconds
            except Exception as e:
                print(f"Error in background refresh: {e}")
                time.sleep(30)

# Global data manager
data_manager = AggressiveDataManager()

# User class
class User(UserMixin):
    def __init__(self, user_data):
        self.id = user_data.get('id')
        self.username = user_data.get('username')
        self.email = user_data.get('email')
        self.role = user_data.get('role', 'user')
        self.is_active = user_data.get('is_active', True)
        self.permissions = user_data.get('permissions', [])

@login_manager.user_loader
def load_user(user_id):
    users = data_manager.get_users()
    user_data = users.get(str(user_id))
    if user_data:
        return User(user_data)
    return None

# Decorators
def admin_required(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not current_user.is_authenticated or current_user.role != 'admin':
            return jsonify({'error': 'Admin access required'}), 403
        return f(*args, **kwargs)
    return decorated_function

# Aggressive rate limiting
def check_rate_limit_aggressive(user_id, campaign_id=None):
    """Aggressive rate limiting with maximum throughput"""
    current_time = time.time()
    user_data = rate_limit_data[user_id]
    
    # Get campaign-specific rate limits
    campaigns = data_manager.get_campaigns()
    campaign = None
    if campaign_id and str(campaign_id) in campaigns:
        campaign = campaigns[str(campaign_id)]
    
    # Use aggressive defaults
    rate_limits = campaign.get('rate_limits', {}) if campaign else {}
    
    emails_per_second = rate_limits.get('emails_per_second', 100)  # 100 emails/second
    emails_per_minute = rate_limits.get('emails_per_minute', 5000)  # 5000 emails/minute
    emails_per_hour = rate_limits.get('emails_per_hour', 100000)  # 100k emails/hour
    wait_time_between = rate_limits.get('wait_time_between_emails', 0.001)  # 1ms delay
    
    # Check wait time between emails
    if user_id in user_data['last_send_time']:
        time_since_last = current_time - user_data['last_send_time'][user_id]
        if time_since_last < wait_time_between:
            return False, wait_time_between - time_since_last, f"Wait {wait_time_between - time_since_last:.3f} seconds"
    
    # Check limits
    if user_data['second_count'][user_id] >= emails_per_second:
        return False, 1.0, f"Rate limit: {emails_per_second} emails per second"
    
    if user_data['minute_count'][user_id] >= emails_per_minute:
        return False, 60.0, f"Rate limit: {emails_per_minute} emails per minute"
    
    if user_data['hourly_count'][user_id] >= emails_per_hour:
        return False, 3600.0, f"Rate limit: {emails_per_hour} emails per hour"
    
    return True, 0, "OK"

def update_rate_limit_counters_aggressive(user_id):
    """Aggressive rate limit counter updates"""
    current_time = time.time()
    user_data = rate_limit_data[user_id]
    
    user_data['last_send_time'][user_id] = current_time
    user_data['second_count'][user_id] += 1
    user_data['minute_count'][user_id] += 1
    user_data['hourly_count'][user_id] += 1
    user_data['daily_count'][user_id] += 1
    
    # Reset counters
    if current_time - user_data.get('last_second_reset', 0) >= 1:
        user_data['second_count'][user_id] = 1
        user_data['last_second_reset'] = current_time
    
    if current_time - user_data.get('last_minute_reset', 0) >= 60:
        user_data['minute_count'][user_id] = 1
        user_data['last_minute_reset'] = current_time
    
    if current_time - user_data.get('last_hour_reset', 0) >= 3600:
        user_data['hourly_count'][user_id] = 1
        user_data['last_hour_reset'] = current_time
    
    if current_time - user_data.get('last_day_reset', 0) >= 86400:
        user_data['daily_count'][user_id] = 1
        user_data['last_day_reset'] = current_time

# Aggressive email sending
def send_email_aggressive(account, recipient, subject, message, from_name=None, template_id=None, campaign_id=None, user_id=None):
    """Aggressive email sending with minimal overhead"""
    if user_id:
        can_send, wait_time, message = check_rate_limit_aggressive(user_id, campaign_id)
        if not can_send:
            return False, message
        
        update_rate_limit_counters_aggressive(user_id)
    
    try:
        # Implement your email sending logic here
        # This is a placeholder - replace with your actual Zoho email sending code
        print(f"Sending email to {recipient}: {subject}")
        
        # Minimal delay for maximum speed
        time.sleep(0.0001)  # 0.1ms delay
        
        return True, "Email sent successfully"
    except Exception as e:
        print(f"Error sending email to {recipient}: {e}")
        return False, str(e)

# Aggressive campaign execution
def execute_campaign_aggressive_worker(campaign_id, user_id):
    """Aggressive worker function to execute campaign"""
    try:
        campaigns = data_manager.get_campaigns()
        accounts = data_manager.get_accounts()
        
        campaign = campaigns.get(str(campaign_id))
        if not campaign:
            print(f"Campaign {campaign_id} not found")
            return
        
        account_id = campaign.get('account_id')
        account = accounts.get(str(account_id))
        if not account:
            print(f"Account {account_id} not found")
            return
        
        # Get recipients
        data_list_id = campaign.get('data_list_id')
        start_line = campaign.get('start_line', 1)
        
        # Read data list
        data_lists = data_manager.get_data_lists()
        data_list = data_lists.get(str(data_list_id))
        if not data_list:
            print(f"Data list {data_list_id} not found")
            return
        
        # Read emails from file with memory mapping
        filename = data_list.get('filename')
        if not filename or not os.path.exists(filename):
            print(f"Data list file {filename} not found")
            return
        
        # Use memory mapping for all files
        with open(filename, 'r+b') as f:
            mm = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)
            content = mm.read().decode('utf-8')
            mm.close()
            all_lines = content.splitlines()
        
        # Get emails from start_line onwards
        start_index = max(0, start_line - 1)
        if start_index >= len(all_lines):
            print(f"Start line {start_line} is beyond file length")
            return
        
        emails = [line.strip() for line in all_lines[start_index:] if line.strip()]
        
        print(f"Starting campaign {campaign_id} with {len(emails)} emails")
        
        # Update campaign status
        campaign['status'] = 'running'
        campaign['started_at'] = datetime.now().isoformat()
        campaign['total_attempted'] = len(emails)
        campaign['total_sent'] = 0
        campaign['total_failed'] = 0
        
        data_manager.save_campaigns(campaigns)
        
        # Send emails with aggressive batch processing
        batch_size = 200  # Large batches for maximum performance
        
        for i in range(0, len(emails), batch_size):
            batch = emails[i:i + batch_size]
            
            # Check if campaign should be stopped
            current_campaign = data_manager.get_campaigns().get(str(campaign_id))
            if current_campaign and current_campaign.get('status') == 'stopped':
                print(f"Campaign {campaign_id} stopped by user")
                break
            
            # Process batch concurrently with maximum workers
            futures = []
            for email in batch:
                future = email_executor.submit(
                    send_email_aggressive,
                    account, email, campaign['subject'], campaign['message'],
                    campaign.get('from_name'), campaign.get('template_id'),
                    campaign_id, user_id
                )
                futures.append((email, future))
            
            # Collect results
            for email, future in futures:
                try:
                    success, message = future.result(timeout=30)
                    if success:
                        campaign['total_sent'] += 1
                    else:
                        campaign['total_failed'] += 1
                except Exception as e:
                    print(f"Error processing email {email}: {e}")
                    campaign['total_failed'] += 1
            
            # Update campaign every batch
            data_manager.save_campaigns(campaigns)
            socketio.emit('campaign_update', {
                'campaign_id': campaign_id,
                'total_sent': campaign['total_sent'],
                'total_failed': campaign['total_failed'],
                'status': campaign['status']
            })
        
        # Mark campaign as completed
        campaign['status'] = 'completed'
        campaign['completed_at'] = datetime.now().isoformat()
        data_manager.save_campaigns(campaigns)
        
        print(f"Campaign {campaign_id} completed: {campaign['total_sent']} sent, {campaign['total_failed']} failed")
        
    except Exception as e:
        print(f"Error executing campaign {campaign_id}: {e}")
        campaigns = data_manager.get_campaigns()
        if str(campaign_id) in campaigns:
            campaigns[str(campaign_id)]['status'] = 'failed'
            data_manager.save_campaigns(campaigns)

# Routes with aggressive performance
@app.route('/')
@login_required
def dashboard():
    """Dashboard page"""
    return render_template('dashboard.html')

@app.route('/campaigns')
@login_required
def campaigns():
    """Campaigns page with aggressive performance processing"""
    campaigns_data = data_manager.get_campaigns()
    accounts_data = data_manager.get_accounts()
    data_lists = data_manager.get_data_lists()
    
    # Process campaigns efficiently
    campaigns_list = []
    for campaign_id, campaign in campaigns_data.items():
        campaign['id'] = int(campaign_id)
        account_id = campaign.get('account_id')
        campaign['account_name'] = accounts_data.get(str(account_id), {}).get('name', 'Unknown')
        
        # Add data list info
        data_list_id = campaign.get('data_list_id')
        if data_list_id and str(data_list_id) in data_lists:
            campaign['data_list_name'] = data_lists[str(data_list_id)].get('name', 'Unknown')
        
        campaigns_list.append(campaign)
    
    return render_template('campaigns.html', campaigns=campaigns_list, accounts=accounts_data, data_lists=data_lists)

@app.route('/accounts')
@login_required
def accounts():
    """Accounts page"""
    accounts_data = data_manager.get_accounts()
    return render_template('accounts.html', accounts=accounts_data)

@app.route('/users')
@login_required
@admin_required
def users():
    """Users page"""
    users_data = data_manager.get_users()
    return render_template('users.html', users=users_data)

@app.route('/data-lists')
@login_required
def data_lists():
    """Data lists page"""
    data_lists = data_manager.get_data_lists()
    return render_template('data_lists.html', data_lists=data_lists)

# Aggressive API endpoints
@app.route('/api/stats')
@login_required
def api_stats():
    """Aggressive stats API with maximum caching"""
    cache_key = f"stats_{current_user.id}"
    cached_stats = cache.get(cache_key)
    if cached_stats:
        return jsonify(cached_stats)
    
    campaigns = data_manager.get_campaigns()
    accounts = data_manager.get_accounts()
    
    # Calculate stats efficiently
    total_campaigns = len(campaigns)
    active_campaigns = sum(1 for c in campaigns.values() if c.get('status') in ['running', 'ready'])
    total_accounts = len(accounts)
    total_sent = sum(c.get('total_sent', 0) for c in campaigns.values())
    total_failed = sum(c.get('total_failed', 0) for c in campaigns.values())
    
    stats = {
        'total_campaigns': total_campaigns,
        'active_campaigns': active_campaigns,
        'total_accounts': total_accounts,
        'total_sent': total_sent,
        'total_failed': total_failed,
        'delivery_rate': round((total_sent / (total_sent + total_failed) * 100), 1) if (total_sent + total_failed) > 0 else 0,
        'emails_today': total_sent // 30
    }
    
    cache.set(cache_key, stats, ttl=5)  # 5 second cache for maximum speed
    return jsonify(stats)

@app.route('/api/campaigns', methods=['GET', 'POST'])
@login_required
def api_campaigns():
    """Aggressive campaigns API"""
    if request.method == 'GET':
        campaigns_data = data_manager.get_campaigns()
        return jsonify(campaigns_data)
    
    elif request.method == 'POST':
        data = request.get_json()
        
        campaigns = data_manager.get_campaigns()
        campaign_id = str(max([int(k) for k in campaigns.keys()] + [0]) + 1)
        
        new_campaign = {
            'id': int(campaign_id),
            'name': data.get('name'),
            'account_id': data.get('account_id'),
            'data_list_id': data.get('data_list_id'),
            'subject': data.get('subject'),
            'message': data.get('message'),
            'from_name': data.get('from_name'),
            'status': 'ready',
            'created_at': datetime.now().isoformat(),
            'created_by': current_user.id,
            'total_sent': 0,
            'total_failed': 0,
            'total_attempted': 0
        }
        
        campaigns[campaign_id] = new_campaign
        data_manager.save_campaigns(campaigns)
        
        return jsonify({'success': True, 'campaign_id': campaign_id})

@app.route('/api/campaigns/<int:campaign_id>/start', methods=['POST'])
@login_required
def start_campaign(campaign_id):
    """Start campaign with aggressive performance processing"""
    campaigns = data_manager.get_campaigns()
    campaign = campaigns.get(str(campaign_id))
    
    if not campaign:
        return jsonify({'error': 'Campaign not found'}), 404
    
    if campaign.get('status') == 'running':
        return jsonify({'error': 'Campaign is already running'}), 400
    
    # Start campaign in background thread with aggressive processing
    campaign_executor.submit(execute_campaign_aggressive_worker, campaign_id, current_user.id)
    
    return jsonify({'success': True, 'message': 'Campaign started'})

@app.route('/api/campaigns/<int:campaign_id>/stop', methods=['POST'])
@login_required
def stop_campaign(campaign_id):
    """Stop campaign"""
    campaigns = data_manager.get_campaigns()
    campaign = campaigns.get(str(campaign_id))
    
    if not campaign:
        return jsonify({'error': 'Campaign not found'}), 404
    
    campaign['status'] = 'stopped'
    data_manager.save_campaigns(campaigns)
    
    return jsonify({'success': True, 'message': 'Campaign stopped'})

@app.route('/api/accounts', methods=['GET', 'POST'])
@login_required
def api_accounts():
    """Aggressive accounts API"""
    if request.method == 'GET':
        accounts_data = data_manager.get_accounts()
        return jsonify(accounts_data)
    
    elif request.method == 'POST':
        data = request.get_json()
        
        accounts = data_manager.get_accounts()
        account_id = str(max([int(k) for k in accounts.keys()] + [0]) + 1)
        
        new_account = {
            'id': int(account_id),
            'name': data.get('name'),
            'org_id': data.get('org_id'),
            'cookies': data.get('cookies', {}),
            'headers': data.get('headers', {}),
            'created_at': datetime.now().isoformat(),
            'created_by': current_user.id
        }
        
        accounts[account_id] = new_account
        data_manager.save_accounts(accounts)
        
        return jsonify({'success': True, 'account_id': account_id})

@app.route('/api/data-lists', methods=['GET', 'POST'])
@login_required
def api_data_lists():
    """Aggressive data lists API"""
    if request.method == 'GET':
        data_lists = data_manager.get_data_lists()
        return jsonify(data_lists)
    
    elif request.method == 'POST':
        data = request.get_json()
        
        data_lists = data_manager.get_data_lists()
        list_id = str(max([int(k) for k in data_lists.keys()] + [0]) + 1)
        
        new_list = {
            'id': int(list_id),
            'name': data.get('name'),
            'filename': data.get('filename'),
            'created_at': datetime.now().isoformat(),
            'created_by': current_user.id
        }
        
        data_lists[list_id] = new_list
        write_json_file_aggressive(DATA_LISTS_FILE, data_lists)
        
        return jsonify({'success': True, 'list_id': list_id})

# Login routes
@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'POST':
        username = request.form.get('username')
        password = request.form.get('password')
        
        users = data_manager.get_users()
        for user_id, user_data in users.items():
            if user_data.get('username') == username and check_password_hash(user_data.get('password', ''), password):
                user = User(user_data)
                login_user(user)
                return redirect(url_for('dashboard'))
        
        flash('Invalid username or password')
    return render_template('login.html')

@app.route('/logout')
@login_required
def logout():
    logout_user()
    return redirect(url_for('login'))

# Aggressive background tasks
def background_stats_update_aggressive():
    """Aggressive background task to update stats cache"""
    while True:
        try:
            campaigns = data_manager.get_campaigns()
            accounts = data_manager.get_accounts()
            
            # Pre-calculate stats for all users
            total_campaigns = len(campaigns)
            active_campaigns = sum(1 for c in campaigns.values() if c.get('status') in ['running', 'ready'])
            total_accounts = len(accounts)
            total_sent = sum(c.get('total_sent', 0) for c in campaigns.values())
            total_failed = sum(c.get('total_failed', 0) for c in campaigns.values())
            
            stats = {
                'total_campaigns': total_campaigns,
                'active_campaigns': active_campaigns,
                'total_accounts': total_accounts,
                'total_sent': total_sent,
                'total_failed': total_failed,
                'delivery_rate': round((total_sent / (total_sent + total_failed) * 100), 1) if (total_sent + total_failed) > 0 else 0,
                'emails_today': total_sent // 30
            }
            
            # Update cache for all users
            users = data_manager.get_users()
            for user_id in users:
                cache.set(f"stats_{user_id}", stats, ttl=5)
            
            time.sleep(3)  # Update every 3 seconds for maximum speed
        except Exception as e:
            print(f"Error in background stats update: {e}")
            time.sleep(15)

def cleanup_old_data_aggressive():
    """Aggressive cleanup of old data"""
    while True:
        try:
            # Cleanup old rate limit data
            current_time = time.time()
            for user_id in list(rate_limit_data.keys()):
                user_data = rate_limit_data[user_id]
                for key in list(user_data['last_send_time'].keys()):
                    if current_time - user_data['last_send_time'][key] > 86400:
                        del user_data['last_send_time'][key]
            
            # Force garbage collection
            gc.collect()
            
            time.sleep(15)  # Cleanup every 15 seconds
        except Exception as e:
            print(f"Error in cleanup: {e}")
            time.sleep(30)

def resource_utilization_worker_aggressive():
    """Worker to aggressively utilize server resources"""
    while True:
        try:
            # Perform CPU-intensive operations
            for i in range(50000):
                _ = i * i * i
            
            # Perform memory operations
            temp_data = [i for i in range(500000)]
            del temp_data
            
            time.sleep(0.001)  # Minimal delay
        except Exception as e:
            print(f"Error in resource utilization: {e}")
            time.sleep(1)

# Start aggressive background tasks
def start_aggressive_background_tasks():
    """Start all aggressive background tasks"""
    threading.Thread(target=background_stats_update_aggressive, daemon=True).start()
    threading.Thread(target=cleanup_old_data_aggressive, daemon=True).start()
    
    # Start multiple resource utilization workers
    for i in range(100):  # 100 workers to aggressively utilize resources
        threading.Thread(target=resource_utilization_worker_aggressive, daemon=True).start()

# Initialize data files if they don't exist
def init_data_files():
    """Initialize data files with default structure"""
    files = {
        ACCOUNTS_FILE: {},
        CAMPAIGNS_FILE: {},
        USERS_FILE: {
            '1': {
                'id': 1,
                'username': 'admin',
                'password': generate_password_hash('admin'),
                'email': 'admin@example.com',
                'role': 'admin',
                'is_active': True,
                'permissions': ['add_account', 'manage_accounts', 'manage_data', 'view_all_campaigns', 'manage_all_campaigns', 'manage_users', 'view_reports'],
                'created_at': datetime.now().isoformat()
            }
        },
        DATA_LISTS_FILE: {},
        CAMPAIGN_LOGS_FILE: {},
        NOTIFICATIONS_FILE: {}
    }
    
    for filename, default_data in files.items():
        if not os.path.exists(filename):
            write_json_file_aggressive(filename, default_data)

# Signal handlers for graceful shutdown
def signal_handler(signum, frame):
    print("Received shutdown signal, cleaning up...")
    campaign_executor.shutdown(wait=True)
    email_executor.shutdown(wait=True)
    data_executor.shutdown(wait=True)
    stats_executor.shutdown(wait=True)
    file_executor.shutdown(wait=True)
    sys.exit(0)

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# Application startup
if __name__ == '__main__':
    init_data_files()
    start_aggressive_background_tasks()
    
    print("ðŸš€ Starting Aggressive Performance Email Campaign Manager...")
    print(f"ðŸ”¥ Using {CAMPAIGN_WORKERS + EMAIL_WORKERS + DATA_WORKERS + STATS_WORKERS + FILE_WORKERS} total workers")
    print(f"ðŸ’ª Target: 100+ concurrent campaigns, 90-100% CPU usage")
    
    # Run with aggressive performance settings
    socketio.run(
        app,
        host='0.0.0.0',
        port=5000,
        debug=False,
        use_reloader=False,
        threaded=True,
        allow_unsafe_werkzeug=True
    ) 